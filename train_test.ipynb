{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1711865672933
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-31 06:13:59.120035: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-31 06:14:04.142016: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-03-31 06:14:04.142039: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-03-31 06:14:13.054764: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2024-03-31 06:14:13.054850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2024-03-31 06:14:13.054858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "# pytorch 프레임워크 라이브러리\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 자연어 처리 라이브러리\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, BertModel\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "import gluonnlp as nlp\n",
        "\n",
        "# 기타\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1711865821362
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=0,  \n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate    \n",
        "        self.num_classes = num_classes\n",
        "        self.classifier = nn.Linear(hidden_size , self.num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "# 모델 설정 함수\n",
        "def setup_model(model_path, vocab_file, device, num_classes):\n",
        "    bert_model, vocab = get_model_and_vocab(model_path, vocab_file)\n",
        "    tokenizer = nlp.data.BERTSPTokenizer(vocab_file, vocab, lower=False)\n",
        "    model = BERTClassifier(bert_model, num_classes=num_classes, dr_rate=0.5).to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "def get_model_and_vocab(model_path, vocab_file, ctx=\"cpu\"):\n",
        "    bertmodel = BertModel.from_pretrained(model_path, return_dict=False)\n",
        "    device = torch.device(ctx)\n",
        "    bertmodel.to(device)\n",
        "    bertmodel.eval()\n",
        "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
        "        vocab_file, padding_token=\"[PAD]\"\n",
        "    )\n",
        "    return bertmodel, vocab_b_obj\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "    \n",
        "# 데이터 준비 함수\n",
        "def prepare_data(train_df, test_df, tokenizer, max_len, batch_size):\n",
        "\n",
        "    dataset_train = [[row['document'], row['label']] for _, row in train_df.iterrows()]\n",
        "    dataset_test = [[row['document'], row['label']] for _, row in test_df.iterrows()]\n",
        "\n",
        "    data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len, True, False)\n",
        "    data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len, True, False)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "    test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "def logging(log_path, txt):\n",
        "    with open(log_path, 'a') as log_file:\n",
        "        log_file.write(f\"{txt}\\n\")\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1711865822836
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 훈련 함수\n",
        "def train_model(model, train_dataloader, test_dataloader, device, optimizer, scheduler, loss_fn, num_epochs, max_grad_norm, log_interval, model_save_dir):\n",
        "    # early stopping code \n",
        "    best_test_acc = 0.0\n",
        "    best_loss = float('inf')\n",
        "    patience = 5\n",
        "    no_improvement_count = 0\n",
        "    \n",
        "    loss_lst = []\n",
        "    acc_lst = []\n",
        "    \n",
        "    stop_e = 0\n",
        "    log_path = os.path.join(model_save_dir, f\"train_log.txt\")\n",
        "    model_path = os.path.join(model_save_dir, f'movie-review-entiment-analysis-model.pt')\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        # Training and evaluation loop (remains the same)\n",
        "        stop_e += 1 \n",
        "        train_acc = 0.0\n",
        "        test_acc = 0.0\n",
        "        total_loss = 0.0\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "            optimizer.zero_grad()\n",
        "            token_ids = token_ids.long().to(device)\n",
        "            segment_ids = segment_ids.long().to(device)\n",
        "            valid_length = valid_length\n",
        "            label = label.long().to(device)\n",
        "            out = model(token_ids, valid_length, segment_ids)\n",
        "            loss = loss_fn(out, label)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            train_acc += calc_accuracy(out, label)\n",
        "            \n",
        "            if batch_id % log_interval == 0:\n",
        "                print(\"Epoch : {}, Batch ID : {}, Loss {}, Train acc {}\".format(e + 1, batch_id + 1, loss.data.cpu().numpy(), train_acc / (batch_id + 1)))\n",
        "                logging(log_path, f\"Epoch : {e + 1}, Batch ID : {batch_id + 1}, Loss : {loss.data.cpu().numpy()}, Train acc : {train_acc / (batch_id + 1)}\")\n",
        "        \n",
        "        print(f\"Epoch : {e + 1}, Train Accuracy : {train_acc / (batch_id + 1)}\")\n",
        "        logging(log_path, f\"Epoch : {e + 1}, Train Accuracy : {train_acc / (batch_id + 1)}\")\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "                token_ids = token_ids.long().to(device)\n",
        "                segment_ids = segment_ids.long().to(device)\n",
        "                valid_length = valid_length\n",
        "                label = label.long().to(device)\n",
        "                out = model(token_ids, valid_length, segment_ids)\n",
        "                loss = loss_fn(out, label)\n",
        "                test_acc += calc_accuracy(out, label)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        current_test_acc = test_acc / (batch_id + 1)\n",
        "        current_loss = total_loss / (batch_id + 1)\n",
        "        acc_lst.append(current_test_acc)\n",
        "        loss_lst.append(current_loss)\n",
        "        total_time = time.time()-start_time\n",
        "        times = str(timedelta(seconds=total_time))\n",
        "        short = times.split(\".\")[0]\n",
        "        print(\"=\"*100)\n",
        "        print(f\"Epoch : {e + 1} Test Accuracy {current_test_acc}, Loss : {current_loss}\")\n",
        "        print(f\"Epoch : {e + 1}, Train time {short}\")\n",
        "        print(\"=\"*100)\n",
        "        logging(log_path, f\"Epoch : {e + 1} Test Accuracy {current_test_acc}, Loss : {current_loss}\")\n",
        "        logging(log_path, f\"Epoch : {e + 1}, Train time {short}\")\n",
        "            \n",
        "        # Model saving and early stopping\n",
        "        if current_test_acc > best_test_acc and current_loss < best_loss:\n",
        "            best_test_acc = current_test_acc\n",
        "            best_loss = current_loss\n",
        "            no_improvement_count = 0\n",
        "            logging(log_path, f\"Epoch : {e + 1}, Accuracy : {best_test_acc:.2f}, Loss : {best_loss:.2f}\")\n",
        "                \n",
        "            # Save the model\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(f'Model saved: {model_path}')\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "\n",
        "        if no_improvement_count >= patience:\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            logging(log_path, f\"Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "    \n",
        "    return stop_e , best_test_acc, best_loss, log_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1711865826733
        }
      },
      "outputs": [],
      "source": [
        "# 메인 함수\n",
        "def main(train_df, test_df):\n",
        "    # 기본 파라미터 설정\n",
        "    max_len=256\n",
        "    batch_size=32\n",
        "    warmup_ratio=0.1\n",
        "    num_epochs=50\n",
        "    max_grad_norm=1\n",
        "    log_interval=100\n",
        "    learning_rate=5e-5\n",
        "    num_classes = 2 # 분류 할 클래스의 개수\n",
        "\n",
        "    # 모델 로드 및 데이터 준비\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    base_model_path = '../kobert-base-v1'\n",
        "    vocab_file = '../kobert-base-v1/kobert_news_wiki_ko_cased-1087f8699e.spiece'\n",
        "\n",
        "    model, tokenizer = setup_model(base_model_path, vocab_file, device, num_classes)\n",
        "\n",
        "    train_dataloader, test_dataloader = prepare_data(train_df, test_df, tokenizer, max_len, batch_size)\n",
        "    \n",
        "    # 옵티마이저 및 스케쥴러 설정\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    t_total = len(train_dataloader) * num_epochs\n",
        "    warmup_step = int(t_total * warmup_ratio)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "    # 모델 저장 디렉토리 지정 및 생성\n",
        "    current_time = datetime.now()\n",
        "    folder_name = current_time.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "    model_save_dir = os.path.join(f\"./model-save/{folder_name}\")\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    # 모델 훈련\n",
        "    start_time = time.time()\n",
        "    stop_epoch, best_test_acc, best_loss, log_path = train_model(model, \n",
        "                                                                 train_dataloader, \n",
        "                                                                 test_dataloader, \n",
        "                                                                 device, \n",
        "                                                                 optimizer, \n",
        "                                                                 scheduler, \n",
        "                                                                 loss_fn, \n",
        "                                                                 num_epochs, \n",
        "                                                                 max_grad_norm, \n",
        "                                                                 log_interval, \n",
        "                                                                 model_save_dir)\n",
        "    total_time = time.time()-start_time\n",
        "    times = str(timedelta(seconds=total_time))\n",
        "\n",
        "    # 훈련시간 출력\n",
        "    short = times.split(\".\")[0]\n",
        "    print(\"=\"*100)\n",
        "    print(f\"Train time : {short}\")\n",
        "    print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1711867364250
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/tmp/ipykernel_4401/1324129750.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d24206bf71694da29ae1d68241c118e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1, Batch ID : 1, Loss 0.7229323387145996, Train acc 0.46875\n",
            "Epoch : 1, Batch ID : 101, Loss 0.657133162021637, Train acc 0.5095915841584159\n",
            "Epoch : 1, Train Accuracy : 0.5258757961783439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4401/1324129750.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49c4b8cb9cc54724ac8e4137db03d924",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 1 Test Accuracy 0.6285377358490566, Loss : 0.6636752609936696\n",
            "Epoch : 1, Train time 0:03:30\n",
            "====================================================================================================\n",
            "Model saved: ./model-save/2024-03-31_06-17/movie-review-entiment-analysis-model.pt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b9e295f07874d6e8473ff156ea712af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 2, Batch ID : 1, Loss 0.680321216583252, Train acc 0.65625\n",
            "Epoch : 2, Batch ID : 101, Loss 0.3569379150867462, Train acc 0.6921410891089109\n",
            "Epoch : 2, Train Accuracy : 0.7298964968152867\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4ef388d3b314b2b818c3f95119166a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 2 Test Accuracy 0.8007075471698113, Loss : 0.4315129023677898\n",
            "Epoch : 2, Train time 0:03:37\n",
            "====================================================================================================\n",
            "Model saved: ./model-save/2024-03-31_06-17/movie-review-entiment-analysis-model.pt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ece794a9656c4dcdac4c227230b3bf3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 3, Batch ID : 1, Loss 0.7076925039291382, Train acc 0.59375\n",
            "Epoch : 3, Batch ID : 101, Loss 0.2127956748008728, Train acc 0.838180693069307\n",
            "Epoch : 3, Train Accuracy : 0.8493232484076433\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4eaea334db7458682ae01ee910c8980",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 3 Test Accuracy 0.8136792452830188, Loss : 0.44933189325175193\n",
            "Epoch : 3, Train time 0:03:37\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed5cb8a951904b66a0a2438c36c9906e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 4, Batch ID : 1, Loss 0.6557847261428833, Train acc 0.75\n",
            "Epoch : 4, Batch ID : 101, Loss 0.26018625497817993, Train acc 0.9053217821782178\n",
            "Epoch : 4, Train Accuracy : 0.8992834394904459\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3ab5f9f7d5d496b9a82085c16db2a3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 4 Test Accuracy 0.8213443396226415, Loss : 0.48590494973479575\n",
            "Epoch : 4, Train time 0:03:37\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93c99fb4b7e641e38d158791721670fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 5, Batch ID : 1, Loss 0.49477463960647583, Train acc 0.78125\n",
            "Epoch : 5, Batch ID : 101, Loss 0.16677634418010712, Train acc 0.9204826732673267\n",
            "Epoch : 5, Train Accuracy : 0.9243630573248408\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab762742f9384c1bbb29ca492e2afef8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 5 Test Accuracy 0.8573113207547169, Loss : 0.48543697532336666\n",
            "Epoch : 5, Train time 0:03:37\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "642cb16934a64f94a31c9dc0a9bb12fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 6, Batch ID : 1, Loss 0.48518186807632446, Train acc 0.875\n",
            "Epoch : 6, Batch ID : 101, Loss 0.6758376955986023, Train acc 0.9554455445544554\n",
            "Epoch : 6, Train Accuracy : 0.9482484076433121\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49580bb80b59475e9070894557dcadb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 6 Test Accuracy 0.8378537735849056, Loss : 0.5812875145237963\n",
            "Epoch : 6, Train time 0:03:37\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3be46fbd287241adbd35c06ec7adc9f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 7, Batch ID : 1, Loss 0.2902156412601471, Train acc 0.90625\n",
            "Epoch : 7, Batch ID : 101, Loss 0.2531392574310303, Train acc 0.9384282178217822\n",
            "Epoch : 7, Train Accuracy : 0.9462579617834395\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61992d5bfd1d4c27b2115093504b3c8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Epoch : 7 Test Accuracy 0.8360849056603774, Loss : 0.6193571731026443\n",
            "Epoch : 7, Train time 0:03:37\n",
            "====================================================================================================\n",
            "Early stopping triggered. Stopping training.\n",
            "====================================================================================================\n",
            "Train time : 0:25:22\n",
            "====================================================================================================\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'desc' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_table(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./nsmc/ratings_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_test \u001b[38;5;241m=\u001b[39m train_test[:\u001b[38;5;28mlen\u001b[39m(train_test)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m30\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_test\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[14], line 67\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 모델 디스크립션 (모델 등록용)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m desc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Train data version : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(train_version)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m desc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Label data version : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(label_version)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m desc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Embedding data version : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(embed_version)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'desc' referenced before assignment"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_table('./ratings_train.txt')\n",
        "train_df = train_df[:len(train_df)//30]\n",
        "train_test = pd.read_table('./ratings_test.txt')\n",
        "train_test = train_test[:len(train_test)//30]\n",
        "\n",
        "main(train_df,train_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1711865789612
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>6723963</td>\n",
              "      <td>내 셀카가 더 잼있다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>3683108</td>\n",
              "      <td>도랏나 ㅡㅡ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>6406669</td>\n",
              "      <td>황우슬혜랑 차인표가 주인공이냐 이 말같지도 않은 시트콤이? 맨날 내용이 산으로가ㅋ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>6878666</td>\n",
              "      <td>이혜영 강남에서,10여년전에 한번 봤는데,007처럼,은색 재규어 몰고 다니더라.방송...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>164028</td>\n",
              "      <td>저두 별주기가 아까움..무슨 내용인지..쩝,ㅠ.ㅠ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                                           document  label\n",
              "0      9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1      3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2     10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3      9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4      6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
              "...        ...                                                ...    ...\n",
              "4995   6723963                                        내 셀카가 더 잼있다      0\n",
              "4996   3683108                                             도랏나 ㅡㅡ      0\n",
              "4997   6406669      황우슬혜랑 차인표가 주인공이냐 이 말같지도 않은 시트콤이? 맨날 내용이 산으로가ㅋ      0\n",
              "4998   6878666  이혜영 강남에서,10여년전에 한번 봤는데,007처럼,은색 재규어 몰고 다니더라.방송...      0\n",
              "4999    164028                        저두 별주기가 아까움..무슨 내용인지..쩝,ㅠ.ㅠ      0\n",
              "\n",
              "[5000 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
